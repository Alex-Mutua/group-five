{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup \u001b[38;5;28;01mas\u001b[39;00m bs\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from requests import get\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import streamlit.components.v1 as components\n",
    "\n",
    "\n",
    "st.markdown(\"<h1 style='text-align: center; color: black;'>GROUP FIVE SCRAPER APP</h1>\", unsafe_allow_html=True)\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "In this app performs webscraping of data from expat-dakar over multiples pages. And we can also\n",
    "download scraped data from the app directly without scraping them.\n",
    "* **Python libraries:** base64, pandas, streamlit, requests, bs4\n",
    "* **Data source:** [Expat-Dakar](https://www.expat-dakar.com/).\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# Web scraping of Rental-appartment data on expat-dakar\n",
    "@st.cache_data\n",
    "\n",
    "def convert_df(df):\n",
    "# IMPORTANT: Cache the conversion to prevent computation on every rerun\n",
    "    return df.to_csv().encode('utf-8')\n",
    "def load(dataframe, title, key, key1) :\n",
    "    st.markdown(\"\"\"\n",
    "    <style>\n",
    "    div.stButton {text-align:center}\n",
    "    </style>\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    if st.button(title,key1):\n",
    "        # st.header(title)\n",
    "\n",
    "        st.subheader('Display data dimension')\n",
    "        st.write('Data dimension: ' + str(dataframe.shape[0]) + ' rows and ' + str(dataframe.shape[1]) + ' columns.')\n",
    "        st.dataframe(dataframe)\n",
    "\n",
    "        csv = convert_df(dataframe)\n",
    "\n",
    "        st.download_button(\n",
    "            label=\"Download data as CSV\",\n",
    "            data=csv,\n",
    "            file_name='Data.csv',\n",
    "            mime='text/csv',\n",
    "            key = key)\n",
    "\n",
    "def local_css(file_name):\n",
    "    with open(file_name) as f:\n",
    "        st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "# Fonction for web scraping Rental appartment 1\n",
    "def load_Appartment1_data(mul_page):\n",
    "    # create a empty dataframe df\n",
    "    df = pd.DataFrame()\n",
    "    # loop over pages indexes\n",
    "    for pages in range(1, int(mul_page)+1):\n",
    "        url = f'https://www.expat-dakar.com/appartements-a-louer?page={pages}'\n",
    "        res_c = requests.get(url)  # Fetch the page\n",
    "        soup_c = bs(res_c.text, 'html.parser')  # Parse the HTML content\n",
    "        containers=soup.find_all(\"div\", class_=\"listings-cards__list-item\")\n",
    "\n",
    "        data = []\n",
    "        for container in containers:  # Loop through each listing\n",
    "            try:\n",
    "                # Get the listing link\n",
    "                link = container.find('a')['href']\n",
    "                res_c = requests.get(link)  # Fetch the listing details page\n",
    "                soup_c = bs(res_c.text, 'html.parser')  # Parse the listing details page\n",
    "                \n",
    "                # Extract details\n",
    "                Details = soup_c.find('div', class_=\"listing-item__details\").text.split()[6]\n",
    "                \n",
    "                # Extract area\n",
    "                Area = soup_c.find(\"dd\", class_=\"listing-item__properties__description\").text.strip().replace(\"m²\", \"\")\n",
    "                \n",
    "                # Extract address\n",
    "                Adress = soup_c.find(\"div\", class_=\"listing-item__address\").text.strip().replace(\"\\n\", \"\").replace(\"                           \", \"\")\n",
    "                \n",
    "                # Extract price\n",
    "                Price = soup_c.find(\"span\", class_=\"listing-card__price__value\").text.strip().replace(\"\\u202f\", \"\").replace(\"F Cfa\", \"\")\n",
    "                \n",
    "                # Extract image link\n",
    "                ImageLink = soup_c.find(\"div\", class_=\"gallery__image__inner\").img[\"srcset\"]\n",
    "                \n",
    "                # Append the extracted data as a dictionary\n",
    "                dic = {\n",
    "                    'Details': Details,\n",
    "                    'Area': Area,\n",
    "                    'Adress': Adress,\n",
    "                    'Price': Price,\n",
    "                    'ImageLink': ImageLink\n",
    "                }\n",
    "                data.append(dic)\n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "            return df \n",
    "\n",
    "        # Convert data to DataFrame and concatenate with the main DataFrame\n",
    "        DF = pd.DataFrame(data)\n",
    "        df = pd.concat([df, DF], axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def load_Appartment_2_data(mul_page):\n",
    "    df = pd.DataFrame()\n",
    "    for pages in range (1,int(mul_page)+1):\n",
    "        url = f'https://www.expat-dakar.com/appartements-meubles?page={pages}'\n",
    "        res_c = requests.get(url)  # Fetch the page\n",
    "        soup_c = bs(res_c.text, 'html.parser')  # Parse the HTML content\n",
    "        containers=soup.find_all(\"div\", class_=\"listings-cards__list-item\")\n",
    "\n",
    "        data = []  # Temporary list to store data from the current page\n",
    "\n",
    "        for container in containers:  # Loop through each listing\n",
    "            try:\n",
    "                # Get the listing link\n",
    "                link = container.find('a')['href']\n",
    "                res_c = requests.get(link)  # Fetch the listing details page\n",
    "                soup_c = bs(res_c.text, 'html.parser')  # Parse the listing details page\n",
    "                \n",
    "                # Extract details\n",
    "                Details = soup_c.find('div', class_=\"listing-item__details\").text.split()[6]\n",
    "                \n",
    "                # Extract area\n",
    "                Area = soup_c.find(\"dd\", class_=\"listing-item__properties__description\").text.strip().replace(\"m²\", \"\")\n",
    "                \n",
    "                # Extract address\n",
    "                Adress = soup_c.find(\"div\", class_=\"listing-item__address\").text.strip().replace(\"\\n\", \"\").replace(\"                           \", \"\")\n",
    "                \n",
    "                # Extract price\n",
    "                Price = soup_c.find(\"span\", class_=\"listing-card__price__value\").text.strip().replace(\"\\u202f\", \"\").replace(\"F Cfa\", \"\")\n",
    "                \n",
    "                # Extract image link\n",
    "                ImageLink = soup_c.find(\"div\", class_=\"gallery__image__inner\").img[\"srcset\"]\n",
    "                \n",
    "                # Append the extracted data as a dictionary\n",
    "                dic = {\n",
    "                    'Details': Details,\n",
    "                    'Area': Area,\n",
    "                    'Adress': Adress,\n",
    "                    'Price': Price,\n",
    "                    'ImageLink': ImageLink\n",
    "                }\n",
    "                data.append(dic)\n",
    "            except:\n",
    "            \n",
    "                pass\n",
    "\n",
    "        # Convert data to DataFrame and concatenate with the main DataFrame\n",
    "        DF = pd.DataFrame(data)\n",
    "        df = pd.concat([df, DF], axis=0).reset_index(drop=True)\n",
    "    return df   \n",
    "\n",
    "\n",
    "def load_Appartment_3_data(mul_page):\n",
    "    df = pd.DataFrame() \n",
    "    # loop over pages indexes\n",
    "    for p in range (1,int(mul_page)+1):\n",
    "        url = f'https://www.expat-dakar.com/terrains-a-vendre?page={p}'\n",
    "        res_c = requests.get(url)  # Fetch the page\n",
    "        soup_c = bs(res_c.text, 'html.parser')  # Parse the HTML content\n",
    "        containers = soup_c.find_all(\"div\", class_=\"listings-cards__list-item\")  # Extract listings\n",
    "\n",
    "        data = []  # Temporary list to store data from the current page\n",
    "\n",
    "        for container in containers:  # Loop through each listing\n",
    "            try:\n",
    "                # Get the listing link\n",
    "                link = container.find('a')['href']\n",
    "                res_c = requests.get(link)  # Fetch the listing details page\n",
    "                soup_c = bs(res_c.text, 'html.parser')  # Parse the listing details page\n",
    "                \n",
    "                # Extract details\n",
    "                Details = soup_c.find('div', class_=\"listing-item__details\").text.split()[6]\n",
    "                \n",
    "                # Extract area\n",
    "                Area = soup_c.find(\"dd\", class_=\"listing-item__properties__description\").text.strip().replace(\"m²\", \"\")\n",
    "                \n",
    "                # Extract address\n",
    "                Adress = soup_c.find(\"div\", class_=\"listing-item__address\").text.strip().replace(\"\\n\", \"\").replace(\"                           \", \"\")\n",
    "                \n",
    "                # Extract price\n",
    "                Price = soup_c.find(\"span\", class_=\"listing-card__price__value\").text.strip().replace(\"\\u202f\", \"\").replace(\"F Cfa\", \"\")\n",
    "                \n",
    "                # Extract image link\n",
    "                ImageLink = soup_c.find(\"div\", class_=\"gallery__image__inner\").img[\"srcset\"]\n",
    "                \n",
    "                # Append the extracted data as a dictionary\n",
    "                dic = {\n",
    "                    'Details': Details,\n",
    "                    'Area': Area,\n",
    "                    'Adress': Adress,\n",
    "                    'Price': Price,\n",
    "                    'ImageLink': ImageLink\n",
    "                }\n",
    "                data.append(dic)\n",
    "            except Exception as e:\n",
    "                # Optionally, you can print the error for debugging:\n",
    "                # print(f\"Error processing container: {e}\")\n",
    "                pass\n",
    "\n",
    "        # Convert data to DataFrame and concatenate with the main DataFrame\n",
    "        DF = pd.DataFrame(data)\n",
    "        df = pd.concat([df, DF], axis=0).reset_index(drop=True)\n",
    "    return df   \n",
    "\n",
    "st.sidebar.header('User Input Features')\n",
    "Pages = st.sidebar.selectbox('Pages indexes', list([int(pages) for pages in np.arange(1, 123)]))\n",
    "Choices = st.sidebar.selectbox('Options', ['Scrape the data using beautifulSoup', 'Download scraped data', 'Dashbord of the data',  'Fill the form'])\n",
    "\n",
    "local_css('Group_5.css')  \n",
    "\n",
    "if Choices=='Scrape the data using beautifulSoup':\n",
    "\n",
    "    Appartment_1_data_mul_pag = load_Appartment1_data(Pages)\n",
    "    Appartment_2_data_mul_pag = load_Appartment_2_data(Pages)\n",
    "    Appartment_3_data_mul_pag = load_Appartment_3_data(Pages)\n",
    "\n",
    "\n",
    "    load(Appartment_1_data_mul_pag, 'Rental_Appartment_1 data', '1', '101')\n",
    "    load(Appartment_2_data_mul_pag, 'Rental_Appartment_2 data', '2', '102')\n",
    "    load(Appartment_3_data_mul_pag, 'Rental_Appartment_3 data', '3', '103')\n",
    "elif Choices == 'Download the scraped data': \n",
    "    Appartment_1_data = pd.read_csv('Apartment_1.csv')\n",
    "    Appartment_2_data = pd.read_csv('Apartment _2.csv')\n",
    "    Appartment_3_data = pd.read_csv('Apartment_3.csv') \n",
    "\n",
    "    load(Appartment_1_data, 'Rental_Appartment_1_Data', '1', '101')\n",
    "    load(Appartment_2_data, 'Rental_Appartment_2_Data', '2', '102')\n",
    "    load(Appartment_3_data, 'Rental_Appartment_3_Data', '3', '103') \n",
    "\n",
    "elif  Choices == 'Dashbord of the data': \n",
    "    df1 = pd.read_csv('Apartment _1.csv')\n",
    "    df2 = pd.read_csv('Apartment _2.csv')\n",
    "    df3 = pd.read_csv('Apartment _3.csv')\n",
    "\n",
    "    col1, col2= st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    plot1 = plt.figure(figsize=(11, 7))\n",
    "    color = (0.2,  \n",
    "             0.4,  \n",
    "             0.2,  \n",
    "             0.6) \n",
    "    plt.bar(df1.Area.value_counts()[:10].index, df1.Area.value_counts()[:10].values, color=color)\n",
    "    plt.title('Top 10 Appartments with highest Area')\n",
    "    plt.xlabel('Area')\n",
    "    st.pyplot(plot1)\n",
    "\n",
    "\n",
    "    with col2:\n",
    "        plot2 = plt.figure(figsize=(11, 7))\n",
    "        color = (0.5,  # red component\n",
    "                0.7,  # green component\n",
    "                0.9,  # blue component\n",
    "                0.6)  # transparency\n",
    "        plt.bar(df2.Area.value_counts()[:7].index, df2.Area.value_counts()[:7].values, color=color)\n",
    "        plt.title('The Top 7 Appartments in Land Comparisson')\n",
    "        plt.xlabel('Area')\n",
    "        st.pyplot(plot2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alex_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
